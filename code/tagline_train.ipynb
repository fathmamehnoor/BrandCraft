{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40859eb6-3cb2-4bee-bad7-7b24f71e79d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/workbench/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "MODEL_NAME = 'distilgpt2' \n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e70f6e2-354c-489e-a545-69851e4cc649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<context>', '<slogan>']}\n"
     ]
    }
   ],
   "source": [
    "# Declare special tokens for padding and separating the context from the slogan:\n",
    "SPECIAL_TOKENS_DICT = {\n",
    "    'pad_token': '<pad>',\n",
    "    'additional_special_tokens': ['<context>', '<slogan>'],\n",
    "}\n",
    "\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Show the full list of special tokens:\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29573679-ccea-4a09-8f7c-3f50657c8c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SloganDataset(Dataset):\n",
    "  def __init__(self, filename, tokenizer, seq_length=64):\n",
    "\n",
    "    context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
    "    slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n",
    "    pad_tkn = tokenizer.pad_token_id\n",
    "    eos_tkn = tokenizer.eos_token_id\n",
    "\n",
    "    self.examples = []\n",
    "    with open(filename) as csvfile:\n",
    "      reader = csv.reader(csvfile)\n",
    "      for row in reader:\n",
    "      \n",
    "        # Build the context and slogan segments:\n",
    "        context = [context_tkn] + tokenizer.encode(row[0], max_length=seq_length//2-1)\n",
    "        slogan = [slogan_tkn] + tokenizer.encode(row[1], max_length=seq_length//2-2) + [eos_tkn]\n",
    "        \n",
    "        # Concatenate the two parts together:\n",
    "        tokens = context + slogan + [pad_tkn] * ( seq_length - len(context) - len(slogan) )\n",
    "\n",
    "        # Annotate each token with its corresponding segment:\n",
    "        segments = [context_tkn] * len(context) + [slogan_tkn] * ( seq_length - len(context) )\n",
    "\n",
    "        # Ignore the context, padding, and <slogan> tokens by setting their labels to -100\n",
    "        labels = [-100] * (len(context)+1) + slogan[1:] + [-100] * ( seq_length - len(context) - len(slogan) )\n",
    "\n",
    "        # Add the preprocessed example to the dataset\n",
    "        self.examples.append((tokens, segments, labels))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.examples)\n",
    "  def __getitem__(self, item):\n",
    "    return torch.tensor(self.examples[item])\n",
    "\n",
    "\n",
    "# Build the dataset and display the dimensions of the 1st batch for verification:\n",
    "slogan_dataset = SloganDataset('/project/data/slogans.csv', tokenizer)\n",
    "print(next(iter(slogan_dataset)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32ec3343-dd7f-437b-9214-6ab0a8eec886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Create data indices for training and validation splits:\n",
    "\n",
    "indices = list(range(len(slogan_dataset)))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "split = math.floor(0.1 * len(slogan_dataset))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Build the PyTorch data loaders:\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(slogan_dataset, batch_size=8, sampler=train_sampler)\n",
    "val_loader = DataLoader(slogan_dataset, batch_size=16, sampler=val_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3fd74a6-541c-4d1a-b317-e3c546924e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\n",
    "\n",
    "  for i in range(epochs):\n",
    "\n",
    "    print('\\n--- Starting epoch #{} ---'.format(i))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # These 2 lists will keep track of the batch losses and batch sizes over one epoch:\n",
    "    losses = []\n",
    "    nums = []\n",
    "\n",
    "    for xb in tqdm(train_dl, desc=\"Training\"):\n",
    "      # Move the batch to the training device:\n",
    "      inputs = xb.to(device)\n",
    "\n",
    "      # Call the model with the token ids, segment ids, and the ground truth (labels)\n",
    "      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
    "      \n",
    "      # Add the loss and batch size to the list:\n",
    "      loss = outputs[0]\n",
    "      losses.append(loss.item())\n",
    "      nums.append(len(xb))\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      model.zero_grad()\n",
    "\n",
    "    # Compute the average cost over one epoch:\n",
    "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
    "\n",
    "\n",
    "    # Now do the same thing for validation:\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      losses = []\n",
    "      nums = []\n",
    "\n",
    "      for xb in tqdm(val_dl, desc=\"Validation\"):\n",
    "        inputs = xb.to(device)\n",
    "        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
    "        losses.append(outputs[0].item())\n",
    "        nums.append(len(xb))\n",
    "\n",
    "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
    "\n",
    "    print('\\n--- Epoch #{} finished --- Training cost: {} / Validation cost: {}'.format(i, train_cost, val_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f499e57f-f18b-4039-8809-f7ab1f57a7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting epoch #0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1071/1071 [02:56<00:00,  6.07it/s]\n",
      "Validation: 100%|██████████| 60/60 [00:05<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch #0 finished --- Training cost: 4.044346572526919 / Validation cost: 3.508957450129405\n",
      "\n",
      "--- Starting epoch #1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1071/1071 [02:57<00:00,  6.04it/s]\n",
      "Validation: 100%|██████████| 60/60 [00:05<00:00, 10.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch #1 finished --- Training cost: 2.9140877714789326 / Validation cost: 3.6901600200588964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Move the model to the GPU:\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tune GPT2 for two epochs:\n",
    "optimizer = AdamW(model.parameters())\n",
    "fit(model, optimizer, train_loader, val_loader, epochs=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f626f846-545d-4c6d-8f42-c12e62da20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "# From HuggingFace, adapted to work with the context/slogan separation:\n",
    "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
    "                    device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in trange(length):\n",
    "\n",
    "            inputs = {'input_ids': generated}\n",
    "            if segments_tokens != None:\n",
    "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\n",
    "\n",
    "\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "\n",
    "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "            for i in range(num_samples):\n",
    "                for _ in set(generated[i].tolist()):\n",
    "                    next_token_logits[i, _] /= repetition_penalty\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: # greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb9fd77-6b84-4246-8a5b-ce3369bab514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " The Home ofVodka.\n",
      " Sets my tea. We like get you back.\n",
      " If the baking is ready to be a man, buy a Bolt.\n",
      " Love life's important after coffee.\n",
      " For a world of Intelligent coffee. seiz through professionals.\n",
      " Every moment has a story in mind.\n",
      " America's home of coffee.\n",
      " Key to family coffee.\n",
      " The coffee line to life.\n",
      " Dtested for generations straight.\n",
      " There's no caffeine in Eastlife & out of home. And there is is a Starbucks.\n",
      " Your home from home.\n",
      " Your home to coffee.\n",
      "  steam. Make it easy!\n",
      " Come on with home.\n",
      " Outdo yourself cold.\n",
      " Coffee with flavor every time by the bean.\n",
      " Medium is power. knock off in families\n",
      " The cheese connected to family.\n",
      " Making coffee easy, easy & tasty.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"Starbucks, coffee chain from Seattle\"\n",
    "\n",
    "context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
    "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n",
    "\n",
    "input_ids = [context_tkn] + tokenizer.encode(context)\n",
    "\n",
    "segments = [slogan_tkn] * 64\n",
    "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\n",
    "\n",
    "input_ids += [slogan_tkn]\n",
    "\n",
    "# Move the model back to the CPU for inference:\n",
    "model.to(torch.device('cpu'))\n",
    "\n",
    "# Generate 20 samples of max length 20\n",
    "generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, num_samples=20)\n",
    "\n",
    "print('\\n\\n--- Generated Slogans ---\\n')\n",
    "\n",
    "for g in generated:\n",
    "  slogan = tokenizer.decode(g.squeeze().tolist())\n",
    "  slogan = slogan.split('<|endoftext|>')[0].split('<slogan>')[1]\n",
    "  print(slogan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41971a52-177f-4628-b949-d464e3ae4e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/tokenizer_config.json',\n",
       " './models/special_tokens_map.json',\n",
       " './models/vocab.json',\n",
       " './models/merges.txt',\n",
       " './models/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./models')\n",
    "\n",
    "# Optionally, save the tokenizer if it was used\n",
    "tokenizer.save_pretrained('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4ca8d-3386-469c-a1dc-949773e51873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
